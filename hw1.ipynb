{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80152ca4",
   "metadata": {},
   "source": [
    "Тексты взяты с dw.com\n",
    "- https://www.dw.com/ru/v-rime-otkrylsja-poslednij-sammit-g20-s-uchastiem-angely-merkel/a-59672066\n",
    "- https://www.dw.com/ru/v-germanii-podschitali-ushherb-dlja-jekonomiki-ot-orgprestupnosti/a-59683646\n",
    "- https://www.dw.com/ru/dzhulian-assanzh-stal-pochetnym-chlenom-nemeckogo-pen-centra/a-59699124\n",
    "- https://www.dw.com/ru/bolee-100-stran-dogovorilis-prekratit-vyrubku-lesov-k-2030-godu/a-59697831\n",
    "- https://www.dw.com/ru/google-iz-za-sankcij-zablokiroval-youtube-kanal-sledstvennogo-komiteta-belarusi/a-59695481\n",
    "- https://www.dw.com/ru/vs-rf-ne-prinjal-isk-detej-gulaga-k-gosdume/a-59694453\n",
    "- https://www.dw.com/ru/v-rezultate-vzryva-v-kabule-pogibli-19-chelovek/a-59694753\n",
    "- https://www.dw.com/ru/advokat-safronova-soobshhil-o-novom-okonchatelnom-obvinenii/a-59694081\n",
    "- https://www.dw.com/ru/amerikanskie-smi-soobshhili-o-narashhivanii-vojsk-rf-u-granicy-s-ukrainoj/a-59693633\n",
    "- https://www.dw.com/ru/pravozashhitniki-pozhalovalis-genprokuroru-frg-na-prestuplenija-belorusskih-silovikov/a-59693676\n",
    "\n",
    "<br>\n",
    "Ключевые слова указаны в соответствующей колонке на каждой из страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7fbc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trekc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import RAKE\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "punctuation += '...' + '—' + '…' + '«»'\n",
    "nltk.download(\"stopwords\")\n",
    "stop = stopwords.words('russian')\n",
    "stop.append('который')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39034745",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5e69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    preprocessed = []\n",
    "    # for texts\n",
    "    # not filtering stopwords\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = [word.lower().strip().strip(punctuation) for word in tokens.split()]\n",
    "        tokens = [token for token in tokens if token != '']\n",
    "        for token in tokens:\n",
    "            preprocessed.append(m.parse(token)[0].normal_form)\n",
    "    \n",
    "    # for keywords\n",
    "    else:\n",
    "        for text in tokens:\n",
    "            temp = []\n",
    "            # т. к. есть нграммы, чтобы привести к начальной форме\n",
    "            # все слова, надо их разделить и лемматизировать каждый токен\n",
    "            for ngram in text.split(', '):\n",
    "                if ' ' in ngram:\n",
    "                    ngram = ngram.split()\n",
    "                    ngrams_list = []\n",
    "                    for gram in ngram:\n",
    "                        # filtering stopwords\n",
    "                        if gram not in stop:\n",
    "                            ngrams_list.append(m.parse(gram)[0].normal_form)\n",
    "                    temp.append(' '.join(ngrams_list))\n",
    "                # for 1 gram ngram\n",
    "                else:\n",
    "                    if ngram not in stop:\n",
    "                        temp.append(m.parse(ngram)[0].normal_form)\n",
    "            preprocessed.append(', '.join(temp))\n",
    "        \n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060e325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_texts = []\n",
    "for text in texts.split('<sep>'):\n",
    "    normalized_texts.append(' '.join(normalize_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e8f192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'основной тема двухдневный встреча пандемия коронавирус и ситуация в мировой экономика страна g20 хотеть добиться 70-процентный вакцинация население в весь мир к сентябрь 2022 год в столица италия в суббота 30 октябрь открыться саммит страна большой двадцатка g20 последний такой встреча с участие и о канцлер фрг ангел меркель angela merkel она прибыть с рим вместе с и.о министр финансы и свой вероятный преемник на пост канцлер олафома шолец olaf scholz оба они стремиться подчеркнуть преемственнос'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_texts[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d168c09",
   "metadata": {},
   "source": [
    "# Cтопслова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3251c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "mykeywords = [\n",
    "    'пандемия, пандемия коронавируса, коронавирус, мировая экономика, экономика, G20, вакцинация, саммит, вакцина, глобальное потепление',\n",
    "    'организованная преступность, банды, преступность, наркотики, мошенничество, преступление, отмывание денег',\n",
    "    'Джулиан Ассанж, свобода слова, содержание под стражей, ПЕН центр, нарушение прав человека, секретная информация, секретные материалы',\n",
    "    'greenpeace, саммит, вырубка лесов, уничтожение лесов, глобальное потепление, исчезновение лесов',\n",
    "    'google, Беларусь, следственный комитет, санкции, блокировка, YouTube, Лукашенко, США',\n",
    "    'ГУЛАГ, иск, верховный суд, конституционный суд, суд, дети ГУЛАГа, репрессии, право',\n",
    "    'взрывы, Кабул, стрельба, атака, военный госпиталь, ранения, перестрелка, Талибан, боевики, Афганистан, террористы',\n",
    "    'Иван Сафронов, расследование, Иван Павлов, адвокат, следствие, журналистика, Паволов, Сафронов, ФСБ, шпионаж, спецслужба',\n",
    "    'российские войска, украинская граница, РФ, Киев, войска, военная техника, Украина, переброска, безопасность, армия',\n",
    "    'Беларусь, пытки, Европейский центр конституционных прав и прав человека, Германия, белорусские силовики, демонстранты, задержания, исчезновения, насилие, человечность, расследование, право'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242a5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keywords.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    file_keywords = f.read()\n",
    "\n",
    "file_keywords = file_keywords.split('<sep>')\n",
    "file_keywords = [re.sub('\\n', '', text) for text in file_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620be686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'саммит двадцатки, большая двадцатка, G20, коронавирус, пандемия, вакцинация от ковида, ковид, мировая экономика'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ae3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_keywords = normalize_text(file_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b825706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_mykeywords = normalize_text(mykeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effcf20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'саммит двадцатка, больший двадцатка, g20, коронавирус, пандемия, вакцинация ковид, ковид, мировой экономика'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleared_keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83484508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пандемия, пандемия коронавирус, коронавирус, мировой экономика, экономика, g20, вакцинация, саммит, вакцина, глобальный потепление'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleared_mykeywords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb37d7",
   "metadata": {},
   "source": [
    "Я возьму объединение кс, потому что при пересечении будет слишком мало совпадений.\n",
    "Правда ухудшится полнота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30da1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "un = []\n",
    "\n",
    "for i, text1 in enumerate(cleared_keywords):\n",
    "    tokens1 = text1.split(', ')\n",
    "    tokens2 = cleared_mykeywords[i].split((', '))\n",
    "    un.append(list(set(tokens1) | set(tokens2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eac104",
   "metadata": {},
   "source": [
    "### Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc835282",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake = RAKE.Rake(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e1379f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['весь мир',\n",
       " 'столица италия',\n",
       " 'рим вместе',\n",
       " 'министр финансы',\n",
       " 'именно достигнуть',\n",
       " 'частность обсуждать',\n",
       " 'шотландский глазго',\n",
       " 'рим проходить',\n",
       " 'рабочий место',\n",
       " 'развивающийся экономика',\n",
       " 'также евросоюз',\n",
       " 'встреча лишь',\n",
       " 'встреча',\n",
       " 'ситуация',\n",
       " 'участие',\n",
       " 'прибыть',\n",
       " 'прошедшее',\n",
       " 'ковид',\n",
       " 'добиваться',\n",
       " '5 градус',\n",
       " 'сравнение',\n",
       " 'солдат',\n",
       " 'недавно',\n",
       " 'полицейский']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_keywords = []\n",
    "for text in normalized_texts:\n",
    "    temp = []\n",
    "    rake_text = rake.run(text, maxWords=2, minFrequency=1)\n",
    "    for keyword in rake_text:\n",
    "        if keyword[1] >= 1:\n",
    "            temp.append(keyword[0])\n",
    "    rake_keywords.append(temp)\n",
    "\n",
    "rake_keywords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc57be",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae6904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87324268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g',\n",
       " 'встреча',\n",
       " 'саммит',\n",
       " 'экономика страна',\n",
       " 'рим',\n",
       " 'строгий',\n",
       " 'год',\n",
       " 'путин',\n",
       " 'мера',\n",
       " 'канцлер',\n",
       " 'италия',\n",
       " 'период',\n",
       " 'си',\n",
       " 'вероятный',\n",
       " 'участник',\n",
       " 'процентный вакцинация',\n",
       " 'двадцатка',\n",
       " 'министр',\n",
       " 'цель',\n",
       " 'вакцина',\n",
       " 'открываться всемирный конференция',\n",
       " 'ожесточённый',\n",
       " 'подчеркнуть',\n",
       " 'политика',\n",
       " 'порядок']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank_keywords = []\n",
    "for text in normalized_texts:\n",
    "    temp = []\n",
    "    textrank_text = keywords.keywords(text, language='russian', additional_stopwords=stop, scores=True)\n",
    "    for keyword in textrank_text:\n",
    "        temp.append(keyword[0])\n",
    "    textrank_keywords.append(temp)\n",
    "\n",
    "textrank_keywords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af698057",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06c834ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137ef6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop, smooth_idf=True,\n",
    "                             use_idf=True, ngram_range=(1, 3))\n",
    "\n",
    "X = vectorizer.fit_transform(normalized_texts)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1fd4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# стырено из интернетов, убирать подсказки не буду, чтобы было проще понять\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    \"\"\"Sort a dict with highest score\"\"\"\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature, score\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_keywords(vectorizer, feature_names, doc, average_keywords_len):\n",
    "    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    \n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only TOP_K_KEYWORDS\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items, average_keywords_len)\n",
    "    \n",
    "    return list(keywords.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0759e4",
   "metadata": {},
   "source": [
    "### Я ограничиваю кол-во тэгов для TF IDF средней длинной списка тэгов других экстракторов \n",
    "(так как он TF IDF может выдать вообще бесконечное количество, мое решение подходит для сравнение экстраткоров)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d59ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rakelen = 0\n",
    "for text in rake_keywords:\n",
    "    average_rakelen += len(text)\n",
    "average_rakelen = average_rakelen / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa30d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_textranklen = 0\n",
    "for text in textrank_keywords:\n",
    "    average_textranklen += len(text)\n",
    "average_textranklen = average_textranklen / 10\n",
    "average_keywords_len = (average_textranklen + average_rakelen) / 2\n",
    "average_keywords_len = round(average_keywords_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee811d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g20',\n",
       " 'встреча',\n",
       " 'саммит',\n",
       " 'рим',\n",
       " 'коронавирус',\n",
       " 'страна',\n",
       " 'экономика',\n",
       " 'мир',\n",
       " 'мера',\n",
       " 'экономика страна g20',\n",
       " 'цель',\n",
       " 'тема двухдневный',\n",
       " 'строгий',\n",
       " 'страна g20',\n",
       " 'ситуация мировой экономика',\n",
       " 'ситуация мировой',\n",
       " 'ситуация',\n",
       " 'процентный',\n",
       " 'полицейский',\n",
       " 'пандемия коронавирус ситуация',\n",
       " 'пандемия коронавирус',\n",
       " 'пандемия',\n",
       " 'основной тема двухдневный',\n",
       " 'основной тема',\n",
       " 'основной']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_keywords = []\n",
    "for i, doc in enumerate(normalized_texts):\n",
    "    temp = get_keywords(vectorizer, feature_names, doc, average_keywords_len)\n",
    "    tfidf_keywords.append(temp)\n",
    "\n",
    "tfidf_keywords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b82073",
   "metadata": {},
   "source": [
    "### Шаблоны \n",
    "Посмотрел на них глазками, выделил самое подходящее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cfb29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    ['NOUN', 'NOUN'],\n",
    "    ['ADJF', 'NOUN'],\n",
    "    ['INFN', 'NOUN'],\n",
    "    ['NOUN']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8c0f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pattern(keywords):\n",
    "    pattern_ngrams = []\n",
    "\n",
    "    for text in keywords:\n",
    "        ngrams_list = []\n",
    "        for ngram in text:\n",
    "            checklist = []\n",
    "            if ' ' in ngram:\n",
    "                ngram = ngram.split()\n",
    "                for gram in ngram:\n",
    "                    checklist.append(m.parse(gram)[0].tag.POS)\n",
    "            else:\n",
    "                checklist.append(m.parse(ngram)[0].tag.POS)\n",
    "                \n",
    "            if checklist in patterns:\n",
    "                ngrams_list.append(ngram)\n",
    "        \n",
    "        temp = []\n",
    "        for ngram in ngrams_list:\n",
    "            if isinstance(ngram, list):\n",
    "                temp.append(' '.join(ngram))\n",
    "            else:\n",
    "                temp.append(ngram)\n",
    "        pattern_ngrams.append(temp)\n",
    "    \n",
    "    return pattern_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59de0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_pattern = apply_pattern(un)\n",
    "tfidf_keywords_pattern = apply_pattern(tfidf_keywords)\n",
    "textrank_keywords_pattern = apply_pattern(textrank_keywords)\n",
    "rake_keywords_pattern = apply_pattern(rake_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eca13b",
   "metadata": {},
   "source": [
    "# Скоринг\n",
    "### Считаются все модельные тэги, которые попали в ручные тэги. \n",
    "Проблема - в теории модель может предсказать все слова, и тогда скор будет большой - но так не бывает.\n",
    "Плюс - улучшается качество, я не вижу особо проблем в большом количестве тэгов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c67fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(keywords, model_result):\n",
    "\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    \n",
    "    for i, text in enumerate(model_result):\n",
    "        score = 0\n",
    "        for token in text:\n",
    "            if token in un[i]:\n",
    "                score += 1\n",
    "        \n",
    "        try:\n",
    "            recall = round(score / len(keywords[i]), 4)\n",
    "        except ZeroDivisionError:\n",
    "            recall = 0\n",
    "        \n",
    "        try:        \n",
    "            precision = round(score / len(text), 4)\n",
    "        except ZeroDivisionError:\n",
    "            precision = 0\n",
    "        \n",
    "#         print('Text %s number of entries:' % i, score, \n",
    "#               '### precision: ', precision,\n",
    "#               '### recall:', recall)\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "    \n",
    "    total_recall = round(total_recall/len(model_result), 4)\n",
    "    total_precision = round(total_precision/len(model_result), 4)\n",
    "    \n",
    "    print('total recall =', total_recall)\n",
    "    print('total precision =', total_precision)\n",
    "    print('f-score =', round((2*total_recall*total_precision)/(total_precision+total_recall), 4), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40a7e1",
   "metadata": {},
   "source": [
    "### Точность всегда будет меньше, потому что модели предсказывают довольно много слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67113a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE\n",
      "total recall = 0.0851\n",
      "total precision = 0.0432\n",
      "f-score = 0.0573 \n",
      "\n",
      "TEXTRANK\n",
      "total recall = 0.2073\n",
      "total precision = 0.1204\n",
      "f-score = 0.1523 \n",
      "\n",
      "TF IDF\n",
      "total recall = 0.3876\n",
      "total precision = 0.22\n",
      "f-score = 0.2807 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('RAKE')\n",
    "scoring(un, rake_keywords)\n",
    "print('TEXTRANK')\n",
    "scoring(un, textrank_keywords)\n",
    "print('TF IDF')\n",
    "scoring(un, tfidf_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6c173",
   "metadata": {},
   "source": [
    "### Precision выросла, потому что количество предсказанных слов сильно сократилось. Поэтому даже парочка попаданий на 10 текстов дает неплохой показатель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3358917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE\n",
      "total recall = 0.112\n",
      "total precision = 0.068\n",
      "f-score = 0.0846 \n",
      "\n",
      "TEXTRANK\n",
      "total recall = 0.2364\n",
      "total precision = 0.1849\n",
      "f-score = 0.2075 \n",
      "\n",
      "TF IDF\n",
      "total recall = 0.4328\n",
      "total precision = 0.2855\n",
      "f-score = 0.344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('RAKE')\n",
    "scoring(un_pattern, rake_keywords_pattern)\n",
    "print('TEXTRANK')\n",
    "scoring(un_pattern, textrank_keywords_pattern)\n",
    "print('TF IDF')\n",
    "scoring(un_pattern, tfidf_keywords_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5296516",
   "metadata": {},
   "source": [
    "# Проблемы и улучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf475a",
   "metadata": {},
   "source": [
    "Альтернативный вариант паттернов. Подсмотрел идею у друга, но реализовал самостоятельно. Решает проблемы выделения имен людей, латинских слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73dace14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yargy import Parser, rule, and_, not_, or_\n",
    "from yargy.predicates import gram\n",
    "from yargy.pipelines import morph_pipeline\n",
    "from yargy.interpretation import fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36a10504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat(texts):\n",
    "    latins = []\n",
    "    for text in texts:\n",
    "        for tag in text:\n",
    "            if tag.isascii():\n",
    "                latins.append(tag)\n",
    "    return latins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0829770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g20',\n",
       " 'bka',\n",
       " 'wikileaks',\n",
       " 'cop26',\n",
       " 'greenpeace',\n",
       " 'google',\n",
       " 'youtube',\n",
       " 'politico',\n",
       " 'the washington post']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lat(un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a45b8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = fact(\n",
    "    'Name',\n",
    "    ['first', 'last'],\n",
    ")\n",
    "\n",
    "\n",
    "NAME = rule(\n",
    "    gram('Name').interpretation(\n",
    "        Name.first.inflected()\n",
    "    ),\n",
    "    gram('Surn').interpretation(\n",
    "        Name.last.inflected()\n",
    "    ).interpretation(Name\n",
    "    )\n",
    ")\n",
    "\n",
    "LATIN = morph_pipeline(get_lat(un))\n",
    "\n",
    "A_N = rule(gram('ADJF'), gram('NOUN'))\n",
    "N_N = rule(gram('NOUN'), gram('NOUN'))\n",
    "V_N = rule(gram('INFN'), gram('NOUN'))\n",
    "N = rule(gram('NOUN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ddedde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = Parser(\n",
    "    or_(\n",
    "        NAME,\n",
    "        LATIN,\n",
    "        A_N,\n",
    "        N_N,\n",
    "        V_N,\n",
    "        N\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a9fc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_keywords(keywords):\n",
    "    pattern_keywords = []\n",
    "    for text in keywords:\n",
    "        temp = []\n",
    "        for word in text:\n",
    "            for match in grammar.findall(word):\n",
    "                try:\n",
    "                    match.tokens[1]\n",
    "                    temp.append(match.tokens[0].value + ' ' + match.tokens[1].value)\n",
    "                except:\n",
    "                    temp.append(match.tokens[0].value)\n",
    "        \n",
    "        pattern_keywords.append(temp)\n",
    "        \n",
    "    return pattern_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2099b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_pattern2 = get_pattern_keywords(un)\n",
    "tfidf_keywords_pattern2 = get_pattern_keywords(tfidf_keywords)\n",
    "textrank_keywords_pattern2 = get_pattern_keywords(textrank_keywords)\n",
    "rake_keywords_pattern2 = get_pattern_keywords(rake_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddd297",
   "metadata": {},
   "source": [
    "Результат дейстительно чуть лучше. Думаю, на большем корпусе он был бы сильно лучше.<p>\n",
    "Растет точность, потому что количество предсказанных слов сокращается из-за шаблона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66da2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE\n",
      "total recall = 0.112\n",
      "total precision = 0.068\n",
      "f-score = 0.0846 \n",
      "\n",
      "TEXTRANK\n",
      "total recall = 0.2364\n",
      "total precision = 0.1849\n",
      "f-score = 0.2075 \n",
      "\n",
      "TF IDF\n",
      "total recall = 0.4328\n",
      "total precision = 0.2855\n",
      "f-score = 0.344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('RAKE')\n",
    "scoring(un_pattern, rake_keywords_pattern)\n",
    "print('TEXTRANK')\n",
    "scoring(un_pattern, textrank_keywords_pattern)\n",
    "print('TF IDF')\n",
    "scoring(un_pattern, tfidf_keywords_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fac3c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE\n",
      "total recall = 0.1107\n",
      "total precision = 0.0837\n",
      "f-score = 0.0953 \n",
      "\n",
      "TEXTRANK\n",
      "total recall = 0.2372\n",
      "total precision = 0.2332\n",
      "f-score = 0.2352 \n",
      "\n",
      "TF IDF\n",
      "total recall = 0.3905\n",
      "total precision = 0.3031\n",
      "f-score = 0.3413 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('RAKE')\n",
    "scoring(un_pattern2, rake_keywords_pattern2)\n",
    "print('TEXTRANK')\n",
    "scoring(un_pattern2, textrank_keywords_pattern2)\n",
    "print('TF IDF')\n",
    "scoring(un_pattern2, tfidf_keywords_pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1b93d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['применение санкция',\n",
       " 'подавление демократия',\n",
       " 'право человек',\n",
       " 'российский смоленск',\n",
       " 'общий сложность',\n",
       " 'канада',\n",
       " 'принудительный посадка',\n",
       " 'мочь',\n",
       " 'организация',\n",
       " 'соответствие',\n",
       " 'правило',\n",
       " 'компания']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_keywords_pattern2[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817946b4",
   "metadata": {},
   "source": [
    "Посмотрим, что экстракторы выделяют плохо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab70b0e",
   "metadata": {},
   "source": [
    "Тут: составные латинские слова, очень длинные нграммы <br>\n",
    "Как пофиксить - не знаю, только взять более длинные тексты. Увеличить мин кол-во вхождений тогда. Но вот с длинными нграммами сложно - можно увеличить размер нграмм в модели, но она плохо выделяет длинные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9b63f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g20\n",
      "саммит двадцатка\n",
      "вакцинация\n",
      "вакцинация ковид\n",
      "пандемия коронавирус\n",
      "экономика\n",
      "глобальный потепление\n",
      "вакцина\n",
      "саммит\n",
      "пандемия\n",
      "коронавирус\n",
      "больший двадцатка\n",
      "мировой экономика\n",
      "###\n",
      "хорст зеехофер\n",
      "банда\n",
      "bka\n",
      "наркопреступление\n",
      "организовать преступность германия\n",
      "федеральный ведомство уголовный дело\n",
      "отмывание деньга\n",
      "наркотик\n",
      "преступление\n",
      "экономический ущерб\n",
      "преступность\n",
      "мвд германия\n",
      "###\n",
      "секретный информация\n",
      "джулиан ассанж\n",
      "содержание стража\n",
      "пен центр\n",
      "нарушение право человек\n",
      "немецкий пен-центр\n",
      "процесс дело ассанж\n",
      "секретный материал\n",
      "обвинение против ассанж\n",
      "экстрадиция ассанж сша\n",
      "wikileaks\n",
      "свобода слово\n",
      "###\n",
      "исчезновение лес\n",
      "cop26\n",
      "уничтожение лес\n",
      "вырубка лес\n",
      "климатический саммит оон\n",
      "greenpeace\n",
      "глобальный потепление\n",
      "саммит\n",
      "план сохранение лес\n",
      "потепление климат\n",
      "###\n",
      "google\n",
      "блокировка youtube\n",
      "следственный комитет беларусь\n",
      "youtube\n",
      "лукашенко\n",
      "google-аккаунт\n",
      "санкция\n",
      "блокировка\n",
      "беларусь\n",
      "youtube-канал\n",
      "санкция сша против беларусь\n",
      "сша\n",
      "следственный комитет\n",
      "###\n",
      "иск госдума\n",
      "сталинский репрессия\n",
      "верховный суд\n",
      "гулаг\n",
      "иск\n",
      "жертва сталинский репрессия\n",
      "репрессия\n",
      "ребёнок гулаг\n",
      "ребёнок жертва репрессия\n",
      "конституционный суд\n",
      "верховный суд рф\n",
      "###\n",
      "военный госпиталь\n",
      "кабул\n",
      "боевик\n",
      "террорист\n",
      "стрельба\n",
      "теракт кабул\n",
      "взрыв кабул\n",
      "ранение\n",
      "перестрелка\n",
      "взрыв\n",
      "захват власть талиб\n",
      "атака\n",
      "талиб\n",
      "талибан\n",
      "###\n",
      "журналистика\n",
      "следствие\n",
      "сафронов\n",
      "дело сафронов\n",
      "адвокат\n",
      "журналистика россия\n",
      "спецслужба\n",
      "иван павлов\n",
      "расследование\n",
      "павол\n",
      "иван сафронов\n",
      "новый обвинение против сафронов\n",
      "шпионаж\n",
      "фсб\n",
      "бывший журналист сафронов\n",
      "###\n",
      "российский войско граница украина\n",
      "politico\n",
      "the washington post\n",
      "рф\n",
      "ельня\n",
      "войско\n",
      "российский войско\n",
      "армия\n",
      "войско рф граница украина\n",
      "безопасность\n",
      "переброска\n",
      "###\n",
      "пытка\n",
      "исчезновение\n",
      "право\n",
      "задержание\n",
      "человечность\n",
      "европейский центр конституционный право право человек\n",
      "генеральный прокурор германия\n",
      "насилие\n",
      "преступление режим лукашенко\n",
      "германия\n",
      "расследование\n",
      "насилие отношение демонстрант беларусь\n",
      "пытка беларусь\n",
      "преступление белорусский силовик\n",
      "демонстрант\n",
      "белорусский силовик\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(un):\n",
    "    for token in text:\n",
    "        if token not in rake_keywords[i]:\n",
    "            print(token)\n",
    "    print('###')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a6822",
   "metadata": {},
   "source": [
    "Тут примерно то же самое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "593c7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g20\n",
      "саммит двадцатка\n",
      "вакцинация\n",
      "ковид\n",
      "вакцинация ковид\n",
      "пандемия коронавирус\n",
      "экономика\n",
      "глобальный потепление\n",
      "пандемия\n",
      "коронавирус\n",
      "больший двадцатка\n",
      "мировой экономика\n",
      "###\n",
      "хорст зеехофер\n",
      "организовать преступность\n",
      "наркопреступление\n",
      "мошенничество\n",
      "организовать преступность германия\n",
      "федеральный ведомство уголовный дело\n",
      "отмывание деньга\n",
      "преступление\n",
      "экономический ущерб\n",
      "мвд германия\n",
      "###\n",
      "секретный информация\n",
      "джулиан ассанж\n",
      "содержание стража\n",
      "пен центр\n",
      "нарушение право человек\n",
      "немецкий пен-центр\n",
      "процесс дело ассанж\n",
      "секретный материал\n",
      "обвинение против ассанж\n",
      "экстрадиция ассанж сша\n",
      "wikileaks\n",
      "свобода слово\n",
      "###\n",
      "исчезновение лес\n",
      "cop26\n",
      "уничтожение лес\n",
      "климатический саммит оон\n",
      "глобальный потепление\n",
      "климатический саммит\n",
      "план сохранение лес\n",
      "потепление климат\n",
      "###\n",
      "google\n",
      "блокировка youtube\n",
      "следственный комитет беларусь\n",
      "лукашенко\n",
      "google-аккаунт\n",
      "блокировка\n",
      "беларусь\n",
      "youtube-канал\n",
      "санкция сша против беларусь\n",
      "следственный комитет\n",
      "###\n",
      "иск госдума\n",
      "сталинский репрессия\n",
      "верховный суд\n",
      "гулаг\n",
      "жертва сталинский репрессия\n",
      "право\n",
      "ребёнок жертва репрессия\n",
      "конституционный суд\n",
      "верховный суд рф\n",
      "суд\n",
      "###\n",
      "военный госпиталь\n",
      "террорист\n",
      "стрельба\n",
      "теракт кабул\n",
      "взрыв кабул\n",
      "ранение\n",
      "перестрелка\n",
      "афганистан\n",
      "захват власть талиб\n",
      "талиб\n",
      "талибан\n",
      "###\n",
      "журналистика\n",
      "сафронов\n",
      "дело сафронов\n",
      "адвокат\n",
      "журналистика россия\n",
      "иван павлов\n",
      "павол\n",
      "новый обвинение против сафронов\n",
      "фсб\n",
      "бывший журналист сафронов\n",
      "###\n",
      "российский войско граница украина\n",
      "politico\n",
      "the washington post\n",
      "киев\n",
      "ельня\n",
      "украинский граница\n",
      "войско\n",
      "российский войско\n",
      "армия\n",
      "войско рф граница украина\n",
      "безопасность\n",
      "переброска\n",
      "военный техника\n",
      "###\n",
      "пытка\n",
      "исчезновение\n",
      "задержание\n",
      "человечность\n",
      "европейский центр конституционный право право человек\n",
      "генеральный прокурор германия\n",
      "насилие\n",
      "преступление режим лукашенко\n",
      "беларусь\n",
      "насилие отношение демонстрант беларусь\n",
      "пытка беларусь\n",
      "преступление белорусский силовик\n",
      "демонстрант\n",
      "белорусский силовик\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(un):\n",
    "    for token in text:\n",
    "        if token not in textrank_keywords[i]:\n",
    "            print(token)\n",
    "    print('###')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d9a64",
   "metadata": {},
   "source": [
    "TF IDF почему-то плохо выделяет единичные слова.<br>\n",
    "Как пофиксить - увеличить объем данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57933510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "саммит двадцатка\n",
      "вакцинация\n",
      "ковид\n",
      "вакцинация ковид\n",
      "глобальный потепление\n",
      "вакцина\n",
      "больший двадцатка\n",
      "мировой экономика\n",
      "###\n",
      "хорст зеехофер\n",
      "организовать преступность\n",
      "наркопреступление\n",
      "мошенничество\n",
      "организовать преступность германия\n",
      "федеральный ведомство уголовный дело\n",
      "отмывание деньга\n",
      "преступление\n",
      "экономический ущерб\n",
      "преступность\n",
      "мвд германия\n",
      "###\n",
      "секретный информация\n",
      "содержание стража\n",
      "нарушение право человек\n",
      "немецкий пен-центр\n",
      "процесс дело ассанж\n",
      "секретный материал\n",
      "обвинение против ассанж\n",
      "экстрадиция ассанж сша\n",
      "свобода слово\n",
      "###\n",
      "исчезновение лес\n",
      "cop26\n",
      "уничтожение лес\n",
      "климатический саммит оон\n",
      "greenpeace\n",
      "глобальный потепление\n",
      "саммит\n",
      "климатический саммит\n",
      "потепление климат\n",
      "###\n",
      "google\n",
      "блокировка youtube\n",
      "лукашенко\n",
      "google-аккаунт\n",
      "блокировка\n",
      "youtube-канал\n",
      "санкция сша против беларусь\n",
      "сша\n",
      "###\n",
      "иск госдума\n",
      "сталинский репрессия\n",
      "жертва сталинский репрессия\n",
      "право\n",
      "ребёнок жертва репрессия\n",
      "верховный суд рф\n",
      "###\n",
      "террорист\n",
      "стрельба\n",
      "теракт кабул\n",
      "взрыв кабул\n",
      "ранение\n",
      "перестрелка\n",
      "захват власть талиб\n",
      "атака\n",
      "###\n",
      "журналистика\n",
      "дело сафронов\n",
      "журналистика россия\n",
      "иван павлов\n",
      "павол\n",
      "новый обвинение против сафронов\n",
      "фсб\n",
      "бывший журналист сафронов\n",
      "###\n",
      "российский войско граница украина\n",
      "the washington post\n",
      "ельня\n",
      "украинский граница\n",
      "армия\n",
      "войско рф граница украина\n",
      "безопасность\n",
      "###\n",
      "исчезновение\n",
      "задержание\n",
      "европейский центр конституционный право право человек\n",
      "генеральный прокурор германия\n",
      "преступление режим лукашенко\n",
      "насилие отношение демонстрант беларусь\n",
      "пытка беларусь\n",
      "преступление белорусский силовик\n",
      "демонстрант\n",
      "белорусский силовик\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(un):\n",
    "    for token in text:\n",
    "        if token not in tfidf_keywords[i]:\n",
    "            print(token)\n",
    "    print('###')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
